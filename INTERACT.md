Recommended Starter: Ollama. Download from ollama.com and install via a single command (e.g., on macOS/Linux: curl -fsSL https://ollama.com/install.sh | sh; Windows has an installer). Then, pull a model with ollama pull llama3 and run it with ollama run llama3 for a command-line chat. Add Open WebUI for a polished interface: Install Docker, then run docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e OLLAMA_API_BASE=http://host.docker.internal:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main and access at http://localhost:3000.